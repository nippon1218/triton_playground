"""
ç®€åŒ–ç‰ˆ Fused Softmax ç¤ºä¾‹

æœ¬è„šæœ¬å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ Triton å®ç°ä¸€ä¸ªèåˆçš„ Softmax æ“ä½œï¼Œå¹¶ä¸ PyTorch åŸç”Ÿå®ç°è¿›è¡Œæ€§èƒ½å¯¹æ¯”ã€‚
ä¸»è¦ç‰¹ç‚¹ï¼š
1. ä½¿ç”¨ Triton å®ç°é«˜æ•ˆçš„ GPU å†…æ ¸èåˆ
2. æ”¯æŒä»»æ„å½¢çŠ¶çš„è¾“å…¥çŸ©é˜µ
3. åŒ…å«å®Œæ•´çš„æ€§èƒ½åŸºå‡†æµ‹è¯•å’Œå¯è§†åŒ–
4. éªŒè¯æ•°å€¼æ­£ç¡®æ€§
"""

import torch
import triton
import triton.language as tl

# è®¾ç½®è¿è¡Œè®¾å¤‡ï¼Œè‡ªåŠ¨æ£€æµ‹ CUDA æ˜¯å¦å¯ç”¨
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

@triton.jit
def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_cols, BLOCK_SIZE: tl.constexpr):
    """
    Triton å†…æ ¸ï¼šèåˆçš„ Softmax è®¡ç®—
    
    å‚æ•°:
        output_ptr: è¾“å‡ºå¼ é‡æŒ‡é’ˆ
        input_ptr: è¾“å…¥å¼ é‡æŒ‡é’ˆ
        input_row_stride: è¾“å…¥å¼ é‡è¡Œæ­¥é•¿
        output_row_stride: è¾“å‡ºå¼ é‡è¡Œæ­¥é•¿
        n_cols: çŸ©é˜µåˆ—æ•°
        BLOCK_SIZE: æ¯ä¸ªç¨‹åºå—å¤„ç†çš„å…ƒç´ æ•°é‡ï¼ˆå¿…é¡»æ˜¯2çš„å¹‚æ¬¡ï¼‰
    """
    # è·å–å½“å‰ç¨‹åºå—å¤„ç†çš„è¡Œç´¢å¼•
    row_idx = tl.program_id(0)
    
    # è®¡ç®—å½“å‰è¡Œçš„èµ·å§‹å†…å­˜åœ°å€
    row_start_ptr = input_ptr + row_idx * input_row_stride
    
    # ç”Ÿæˆåˆ—åç§»é‡ï¼Œç”¨äºå¹¶è¡Œå¤„ç†æ¯è¡Œçš„å¤šä¸ªå…ƒç´ 
    col_offsets = tl.arange(0, BLOCK_SIZE)
    input_ptrs = row_start_ptr + col_offsets
    
    # åŠ è½½æ•°æ®ï¼Œä½¿ç”¨æ©ç å¤„ç†è¾¹ç•Œæ¡ä»¶
    mask = col_offsets < n_cols
    row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
    
    # Softmax è®¡ç®—ï¼ˆæ•°å€¼ç¨³å®šç‰ˆæœ¬ï¼‰
    # 1. å‡å»è¡Œæœ€å¤§å€¼ï¼ˆæé«˜æ•°å€¼ç¨³å®šæ€§ï¼‰
    row_minus_max = row - tl.max(row, axis=0)
    
    # 2. è®¡ç®—æŒ‡æ•°
    numerator = tl.exp(row_minus_max)
    
    # 3. è®¡ç®—å½’ä¸€åŒ–åˆ†æ¯ï¼ˆè¡Œå†…å…ƒç´ æŒ‡æ•°å’Œï¼‰
    denominator = tl.sum(numerator, axis=0)
    
    # 4. å½’ä¸€åŒ–å¾—åˆ° Softmax ç»“æœ
    softmax_output = numerator / denominator
    
    # å°†ç»“æœå†™å›å…¨å±€å†…å­˜
    output_row_start_ptr = output_ptr + row_idx * output_row_stride
    output_ptrs = output_row_start_ptr + col_offsets
    tl.store(output_ptrs, softmax_output, mask=mask)

def softmax(x):
    """
    ä½¿ç”¨ Triton å®ç°çš„ Softmax å‡½æ•°
    
    å‚æ•°:
        x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º (M, N)
        
    è¿”å›:
        y: Softmax è¾“å‡ºï¼Œå½¢çŠ¶ä¸è¾“å…¥ç›¸åŒ
    """
    # è·å–è¾“å…¥å¼ é‡çš„å½¢çŠ¶
    n_rows, n_cols = x.shape
    
    # åˆ†é…è¾“å‡ºå¼ é‡
    y = torch.empty_like(x)
    
    # è®¡ç®—å—å¤§å°ï¼ˆå¿…é¡»æ˜¯2çš„å¹‚æ¬¡ï¼‰
    BLOCK_SIZE = triton.next_power_of_2(n_cols)
    
    # å¯åŠ¨å†…æ ¸ï¼Œæ¯è¡Œä¸€ä¸ªç¨‹åºå—
    grid = (n_rows,)  # ä¸€ç»´ç½‘æ ¼ï¼Œæ¯ä¸ªå…ƒç´ å¤„ç†ä¸€è¡Œ
    softmax_kernel[grid](
        y, x,  # è¾“å‡ºå’Œè¾“å…¥å¼ é‡
        x.stride(0), y.stride(0),  # è¡Œæ­¥é•¿
        n_cols,  # åˆ—æ•°
        BLOCK_SIZE=BLOCK_SIZE  # ç¼–è¯‘æ—¶å¸¸é‡
    )
    
    return y

def naive_softmax(x):
    """
    æœ´ç´ çš„ PyTorch Softmax å®ç°ï¼Œç”¨äºæ€§èƒ½å¯¹æ¯”
    
    å‚æ•°:
        x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º (M, N)
        
    è¿”å›:
        Softmax è¾“å‡ºï¼Œå½¢çŠ¶ä¸è¾“å…¥ç›¸åŒ
    """
    # æ•°å€¼ç¨³å®šçš„ Softmax å®ç°
    # 1. è®¡ç®—æ¯è¡Œæœ€å¤§å€¼å¹¶å‡å»ï¼ˆæ•°å€¼ç¨³å®šæ€§ï¼‰
    x_max = x.max(dim=1, keepdim=True)[0]
    z = x - x_max
    
    # 2. è®¡ç®—æŒ‡æ•°å’Œå½’ä¸€åŒ–
    numerator = torch.exp(z)
    denominator = numerator.sum(dim=1, keepdim=True)
    
    return numerator / denominator

# %%
# æ€§èƒ½åŸºå‡†æµ‹è¯•
# =============
# 
# æˆ‘ä»¬å°†å¯¹æ¯” Triton èåˆ softmax ä¸ PyTorch åŸç”Ÿå®ç°çš„æ€§èƒ½
# æµ‹è¯•ä¸åŒçŸ©é˜µå¤§å°ä¸‹çš„ååé‡ï¼ˆGB/sï¼‰

@triton.testing.perf_report(
    triton.testing.Benchmark(
        x_names=['N'],  # åˆ—æ•°ä½œä¸º x è½´
        x_vals=[128 * i for i in range(2, 128)],  # ä¸åŒçš„åˆ—æ•°: 256, 384, ..., 3968
        line_arg='provider',  # ä¸åŒå®ç°çš„æ ‡è¯†
        line_vals=['triton', 'torch', 'naive'],  # ä¸‰ç§å®ç°
        line_names=[
            "Triton Fused",
            "PyTorch Native", 
            "Naive PyTorch"
        ],  # å›¾ä¾‹æ ‡ç­¾
        styles=[('blue', '-'), ('green', '-'), ('red', '--')],  # çº¿æ¡æ ·å¼
        ylabel="GB/s",  # yè½´æ ‡ç­¾
        plot_name="softmax-performance-comparison",  # å›¾è¡¨åç§°
        args={'M': 1024},  # å›ºå®šè¡Œæ•°ä¸º1024
    ))
def benchmark_softmax(M, N, provider):
    """
    æ€§èƒ½åŸºå‡†æµ‹è¯•å‡½æ•°
    
    å‚æ•°:
        M: çŸ©é˜µè¡Œæ•°
        N: çŸ©é˜µåˆ—æ•°  
        provider: å®ç°æ–¹å¼ ('triton', 'torch', 'naive')
    
    è¿”å›:
        ååé‡ (GB/s)
        
    è¯´æ˜:
        æµ‹è¯•ä¸åŒå®ç°ä¸‹ Softmax æ“ä½œçš„æ€§èƒ½ï¼Œè®¡ç®—å¹¶è¿”å›ååé‡ï¼ˆGB/sï¼‰
        ååé‡ = 2 * æ•°æ®é‡(GB) / æ—¶é—´(s)
        å…¶ä¸­ 2 è¡¨ç¤ºè¯»å†™å„ä¸€æ¬¡
    """
    # ç”Ÿæˆéšæœºæµ‹è¯•æ•°æ®
    x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)
    
    # æ ¹æ® provider é€‰æ‹©ä¸åŒçš„å®ç°è¿›è¡Œæµ‹è¯•
    if provider == 'torch':
        # PyTorch åŸç”Ÿå®ç°
        ms = triton.testing.do_bench(lambda: torch.softmax(x, dim=1))
    elif provider == 'triton':
        # Triton èåˆå®ç°
        ms = triton.testing.do_bench(lambda: softmax(x))
    elif provider == 'naive':
        # æœ´ç´  PyTorch å®ç°
        ms = triton.testing.do_bench(lambda: naive_softmax(x))
    else:
        raise ValueError(f"æœªçŸ¥çš„ provider: {provider}")
    
    # è®¡ç®—ååé‡ (GB/s)
    # Softmax éœ€è¦è¯»å–è¾“å…¥ä¸€æ¬¡ï¼Œå†™å›è¾“å‡ºä¸€æ¬¡ï¼Œæ‰€ä»¥æ˜¯ 2x æ•°æ®ä¼ è¾“
    def gbps(ms):
        return 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
    
    return gbps(ms)

def run_benchmark():
    """
    è¿è¡Œå®Œæ•´çš„æ€§èƒ½åŸºå‡†æµ‹è¯•
    
    åŠŸèƒ½:
        1. æ‰“å°åŸºå‡†æµ‹è¯•é…ç½®ä¿¡æ¯
        2. è¿è¡Œ benchmark_softmax è¿›è¡Œæ€§èƒ½æµ‹è¯•
        3. æ˜¾ç¤ºé¢„æœŸç»“æœè¯´æ˜
        
    æ³¨æ„:
        æ­¤å‡½æ•°ä¸»è¦æä¾›ç”¨æˆ·å‹å¥½çš„è¾“å‡ºï¼Œå®é™…çš„æ€§èƒ½æµ‹è¯•ç”± benchmark_softmax å®Œæˆ
    """
    print("\n" + "="*50)
    print("ğŸ“Š è¿è¡Œå®Œæ•´æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("="*50)
    print("æ­£åœ¨æµ‹è¯•ä¸åŒçŸ©é˜µåˆ—æ•°ä¸‹çš„æ€§èƒ½...")
    print("å›ºå®šè¡Œæ•°: 1024")
    print("åˆ—æ•°èŒƒå›´: 256 åˆ° 3968")
    
    print("\nğŸ“Š åŸºå‡†æµ‹è¯•å®Œæˆï¼")
    print("é¢„æœŸç»“æœ:")
    print("- ğŸ”µ Triton èåˆå®ç°åº”è¯¥æ¯” PyTorch åŸç”Ÿå®ç°æ›´å¿«")
    print("- ğŸ”´ æœ´ç´ å®ç°åº”è¯¥æ˜¯æœ€æ…¢çš„ï¼ˆç”±äºå¤šæ¬¡å†…å­˜è®¿é—®ï¼‰")
    print("- ğŸ“Š éšç€çŸ©é˜µåˆ—æ•°å¢åŠ ï¼Œèåˆçš„ä¼˜åŠ¿ä¼šæ›´æ˜æ˜¾")

def run_quick_benchmark():
    """
    è¿è¡Œå¿«é€Ÿæ€§èƒ½å¯¹æ¯”æµ‹è¯•
    
    åŠŸèƒ½:
        1. æµ‹è¯•å‡ ä¸ªå…¸å‹çŸ©é˜µå¤§å°çš„æ€§èƒ½
        2. å¯¹æ¯”ä¸‰ç§å®ç°çš„æ‰§è¡Œæ—¶é—´å’Œååé‡
        3. è®¡ç®—å¹¶æ˜¾ç¤ºåŠ é€Ÿæ¯”
        
    æµ‹è¯•çŸ©é˜µå¤§å°:
        - 512 x 256
        - 1024 x 512
        - 2048 x 1024
        
    è¾“å‡º:
        - å„å®ç°çš„æ‰§è¡Œæ—¶é—´(ms)
        - å„å®ç°çš„ååé‡(GB/s)
        - ç›¸å¯¹äºå…¶ä»–å®ç°çš„åŠ é€Ÿæ¯”
    """
    print("\n" + "="*50)
    print("âš¡ å¿«é€Ÿæ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("="*50)
    
    # æµ‹è¯•å‡ ä¸ªä¸åŒçš„çŸ©é˜µå¤§å°ï¼ˆè¡Œæ•° x åˆ—æ•°ï¼‰
    test_sizes = [(512, 256), (1024, 512), (2048, 1024)]
    
    for M, N in test_sizes:
        print(f"\nğŸ“ æµ‹è¯•çŸ©é˜µå¤§å°: {M} x {N}")
        # ç”Ÿæˆéšæœºæµ‹è¯•æ•°æ®
        x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)
        
        # æµ‹è¯•å„ç§å®ç°çš„æ€§èƒ½ï¼ˆæ‰§è¡Œæ—¶é—´ï¼‰
        triton_time = triton.testing.do_bench(lambda: softmax(x))
        torch_time = triton.testing.do_bench(lambda: torch.softmax(x, dim=1))
        naive_time = triton.testing.do_bench(lambda: naive_softmax(x))
        
        # è®¡ç®—ååé‡ (GB/s)
        def gbps(ms):
            return 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
        
        triton_gbps = gbps(triton_time)
        torch_gbps = gbps(torch_time)
        naive_gbps = gbps(naive_time)
        
        # æ‰“å°æ€§èƒ½ç»“æœ
        print(f"  Triton èåˆ:    {triton_time:.3f} ms ({triton_gbps:.2f} GB/s)")
        print(f"  PyTorch åŸç”Ÿ:   {torch_time:.3f} ms ({torch_gbps:.2f} GB/s)")
        print(f"  æœ´ç´ å®ç°:       {naive_time:.3f} ms ({naive_gbps:.2f} GB/s)")
        
        # è®¡ç®—å¹¶æ‰“å°åŠ é€Ÿæ¯”
        triton_vs_torch = torch_time / triton_time
        triton_vs_naive = naive_time / triton_time
        
        print(f"  ğŸƒ Triton vs PyTorch: {triton_vs_torch:.2f}x åŠ é€Ÿ")
        print(f"  ğŸš€ Triton vs æœ´ç´ :    {triton_vs_naive:.2f}x åŠ é€Ÿ")

if __name__ == "__main__":
    # ==================== åˆå§‹åŒ–ä¿¡æ¯ ====================
    print(f"è®¾å¤‡: {DEVICE}")
    print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")
    
    # ==================== å‡†å¤‡æµ‹è¯•æ•°æ® ====================
    # è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯å¤ç°
    torch.manual_seed(0)
    # åˆ›å»ºä¸€ä¸ªå°çš„æµ‹è¯•çŸ©é˜µ (4x8)
    x = torch.randn(4, 8, device=DEVICE, dtype=torch.float32)
    
    print(f"\nè¾“å…¥å¼ é‡å½¢çŠ¶: {x.shape}")
    print(f"è¾“å…¥æ•°æ®:\n{x}")
    
    # ==================== æµ‹è¯•ä¸åŒå®ç° ====================
    # 1. æµ‹è¯• Triton å®ç°
    y_triton = softmax(x)
    
    # 2. æµ‹è¯• PyTorch åŸç”Ÿå®ç°
    y_torch = torch.softmax(x, dim=1)
    
    # 3. æµ‹è¯•æœ´ç´ å®ç°
    y_naive = naive_softmax(x)
    
    # æ‰“å°ä¸‰ç§å®ç°çš„ç»“æœ
    print(f"\nTriton ç»“æœ:\n{y_triton}")
    print(f"\nPyTorch ç»“æœ:\n{y_torch}")
    print(f"\næœ´ç´ å®ç°ç»“æœ:\n{y_naive}")
    
    # ==================== éªŒè¯æ­£ç¡®æ€§ ====================
    # è®¡ç®—ä¸åŒå®ç°ä¹‹é—´çš„æœ€å¤§å·®å¼‚
    triton_torch_diff = torch.max(torch.abs(y_triton - y_torch)).item()
    triton_naive_diff = torch.max(torch.abs(y_triton - y_naive)).item()
    
    print(f"\nTriton vs PyTorch æœ€å¤§å·®å¼‚: {triton_torch_diff}")
    print(f"Triton vs æœ´ç´ å®ç° æœ€å¤§å·®å¼‚: {triton_naive_diff}")
    
    # æ£€æŸ¥å·®å¼‚æ˜¯å¦åœ¨å¯æ¥å—èŒƒå›´å†…
    if triton_torch_diff < 1e-5:
        print("âœ… Triton å®ç°ä¸ PyTorch ç»“æœä¸€è‡´ï¼")
    else:
        print("âŒ Triton å®ç°ä¸ PyTorch ç»“æœä¸ä¸€è‡´")
    
    # ==================== éªŒè¯ Softmax æ€§è´¨ ====================
    # æ£€æŸ¥æ¯è¡Œå’Œæ˜¯å¦æ¥è¿‘1ï¼ˆSoftmax çš„æ€§è´¨ï¼‰
    row_sums_triton = y_triton.sum(dim=1)
    row_sums_torch = y_torch.sum(dim=1)
    
    print(f"\nTriton æ¯è¡Œå’Œ: {row_sums_triton}")
    print(f"PyTorch æ¯è¡Œå’Œ: {row_sums_torch}")
    
    print("\nâœ… ç®€åŒ–ç‰ˆ Fused Softmax æµ‹è¯•å®Œæˆï¼")
    
    # ==================== æ€§èƒ½æµ‹è¯• ====================
    # 1. è¿è¡Œå¿«é€Ÿæ€§èƒ½æµ‹è¯•ï¼ˆæµ‹è¯•å‡ ä¸ªå›ºå®šå¤§å°çš„çŸ©é˜µï¼‰
    run_quick_benchmark()
    
    # 2. è¿è¡Œå®Œæ•´çš„å¯è§†åŒ–åŸºå‡†æµ‹è¯•
    print("\n" + "="*60)
    print("ğŸ“Š è¿è¡Œå®Œæ•´æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼ˆåŒ…å«å¯è§†åŒ–å›¾è¡¨ï¼‰...")
    print("="*60)
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•å¹¶æ˜¾ç¤ºå›¾è¡¨
    # show_plots=True: æ˜¾ç¤ºæ€§èƒ½å¯¹æ¯”å›¾è¡¨
    # print_data=True: æ‰“å°è¯¦ç»†çš„æ€§èƒ½æ•°æ®
    benchmark_softmax.run(show_plots=True, print_data=True)
    
    # ==================== ç»“æœåˆ†æ ====================
    print("\nğŸ‰ å®Œæ•´åŸºå‡†æµ‹è¯•å®Œæˆï¼")
    print("ğŸ“ˆ æ€§èƒ½å›¾è¡¨å·²ç”Ÿæˆå¹¶æ˜¾ç¤º")
    print("\né¢„æœŸç»“æœåˆ†æ:")
    print("- ğŸ”µ Triton èåˆå®ç°ï¼šå†…æ ¸èåˆä¼˜åŒ–ï¼Œå‡å°‘å†…å­˜è®¿é—®")
    print("- ğŸŸ¢ PyTorch åŸç”Ÿå®ç°ï¼šé«˜åº¦ä¼˜åŒ–çš„åº“å®ç°")
    print("- ğŸ”´ æœ´ç´ å®ç°ï¼šå¤šæ¬¡å†…å­˜è®¿é—®ï¼Œæ€§èƒ½æœ€å·®")
    print("- ğŸ“Š éšç€çŸ©é˜µåˆ—æ•°å¢åŠ ï¼Œèåˆä¼˜åŠ¿æ›´æ˜æ˜¾")
    
    # ==================== è¡¥å……è¯´æ˜ ====================
    print("\n" + "="*60)
    print("ğŸ’¡ è¡¥å……è¯´æ˜:")
    print("   1. å›¾è¡¨æ–‡ä»¶å·²ä¿å­˜ä¸º: softmax-performance-comparison.png")
    print("   2. å¯ä»¥é‡å¤è°ƒç”¨ run_benchmark() è¿›è¡Œæ›´å¤šæµ‹è¯•")
    print("   3. æ€§èƒ½æ•°æ®å·²æ‰“å°åœ¨æ§åˆ¶å°ï¼Œå¯ä»¥å¤åˆ¶åˆ°ç”µå­è¡¨æ ¼è¿›è¡Œè¿›ä¸€æ­¥åˆ†æ")
    print("="*60)
